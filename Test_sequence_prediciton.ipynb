{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import pythainlp\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import Word_Embedder\n",
    "from preprocessing import Text_processing\n",
    "from text_classification import Text_classification, Text_classification_for_prediction, count_based_model\n",
    "from text_classification import prepare_data_for_text_classification, prepare_data_for_sequence_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('E:\\\\Coding_projects\\\\data\\\\NER\\\\BEST10\\\\Current\\\\all_without_sw_clean2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = []\n",
    "with open('BEST10_no_sw_tr.txt', 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line != '':\n",
    "            tr.append(line)\n",
    "te = []\n",
    "with open('BEST10_no_sw_te.txt', 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line != '':\n",
    "            te.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17392\n",
      "4020\n"
     ]
    }
   ],
   "source": [
    "print(len(tr))\n",
    "print(len(te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('E:\\\\Coding_projects\\\\Thai_NLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = Text_processing(max_len=64,min_len=5,min_len_character=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tp.preprocessing2(tr,do_padding=True)\n",
    "test = tp.preprocessing2(te, do_padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NR': 0,\n",
       " 'VV': 1,\n",
       " 'NN': 2,\n",
       " 'DDEM': 3,\n",
       " 'JJV': 4,\n",
       " 'P': 5,\n",
       " 'JJA': 6,\n",
       " 'PINT': 7,\n",
       " 'NEG': 8,\n",
       " 'AUX': 9,\n",
       " 'CNJ': 10,\n",
       " 'ADV': 11,\n",
       " 'FXN': 12,\n",
       " 'PDT': 13,\n",
       " 'CL': 14,\n",
       " 'OD': 15,\n",
       " 'COMP': 16,\n",
       " 'REFX': 17,\n",
       " 'VA': 18,\n",
       " 'FXG': 19,\n",
       " 'DINT': 20,\n",
       " 'PPER': 21,\n",
       " 'PDEM': 22,\n",
       " 'CD': 23,\n",
       " 'PAR': 24,\n",
       " 'DPER': 25,\n",
       " 'FXAV': 26,\n",
       " 'FXAJ': 27,\n",
       " 'FWV': 28,\n",
       " 'FWA': 29,\n",
       " 'FWN': 30,\n",
       " 'PU': 31,\n",
       " 'IJ': 32,\n",
       " 'blank': 33,\n",
       " 'FWX': 34,\n",
       " '': 35}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "train[1]['-PAD-'] = len(train[1])\n",
    "tmp = prepare_data_for_sequence_prediction(train,\n",
    "                                           test,\n",
    "                                           max_len=64,\n",
    "                                           min_len=5,\n",
    "                                           unique_labels = train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created the path: trained_models\\sequence_prediction\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word_vectors (InputLayer)       [(None, 64, 300)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masks (InputLayer)              [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 64, 256)      330240      word_vectors[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 64, 256)      296448      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64, 256)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 64, 64)       16448       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 64, 64)       256         time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 64, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64, 64)       0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 64, 32)       2080        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 32)       128         time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 64, 32)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64, 32)       0           time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 64, 21)       693         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 21)       84          time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 64, 21)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64, 21)       0           time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "seq_out (TimeDistributed)       (None, 64, 37)       814         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 647,191\n",
      "Trainable params: 646,957\n",
      "Non-trainable params: 234\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tc = Text_classification(prepared_data_dict=tmp,\n",
    "                         do_deep_learning=True,\n",
    "                         do_linear_classifier=False,\n",
    "                         is_sequence_prediciton=True,\n",
    "                         model_path='sequence_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "272/272 [==============================] - 34s 126ms/step - loss: 3.3345 - val_loss: 2.8356 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "272/272 [==============================] - 29s 108ms/step - loss: 1.5111 - val_loss: 2.5049 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "272/272 [==============================] - 29s 108ms/step - loss: 1.3691 - val_loss: 1.2179 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "272/272 [==============================] - 29s 106ms/step - loss: 1.3194 - val_loss: 0.9848 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "272/272 [==============================] - 28s 103ms/step - loss: 1.2910 - val_loss: 0.9561 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "272/272 [==============================] - 28s 103ms/step - loss: 1.2629 - val_loss: 0.9161 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "272/272 [==============================] - 30s 110ms/step - loss: 1.2459 - val_loss: 0.8958 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "272/272 [==============================] - 39s 142ms/step - loss: 1.2259 - val_loss: 0.8932 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "272/272 [==============================] - 46s 169ms/step - loss: 1.2104 - val_loss: 0.8619 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "272/272 [==============================] - 46s 168ms/step - loss: 1.1961 - val_loss: 0.8544 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "272/272 [==============================] - 46s 168ms/step - loss: 1.1843 - val_loss: 0.8390 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "272/272 [==============================] - 46s 169ms/step - loss: 1.1717 - val_loss: 0.8344 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "272/272 [==============================] - 48s 178ms/step - loss: 1.1597 - val_loss: 0.8447 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "272/272 [==============================] - 52s 190ms/step - loss: 1.1470 - val_loss: 0.8245 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "272/272 [==============================] - 42s 153ms/step - loss: 1.1355 - val_loss: 0.7973 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "272/272 [==============================] - 49s 181ms/step - loss: 1.1237 - val_loss: 0.7963 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "272/272 [==============================] - 45s 166ms/step - loss: 1.1126 - val_loss: 0.8197 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "272/272 [==============================] - 49s 181ms/step - loss: 1.1020 - val_loss: 0.7833 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "272/272 [==============================] - 52s 190ms/step - loss: 1.0893 - val_loss: 0.7702 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "272/272 [==============================] - 46s 167ms/step - loss: 1.0804 - val_loss: 0.7727 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "272/272 [==============================] - 52s 190ms/step - loss: 1.0736 - val_loss: 0.7498 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "272/272 [==============================] - 46s 170ms/step - loss: 1.0642 - val_loss: 0.7638 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "272/272 [==============================] - 51s 188ms/step - loss: 1.0555 - val_loss: 0.7309 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "272/272 [==============================] - 51s 187ms/step - loss: 1.0478 - val_loss: 0.7456 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "272/272 [==============================] - 45s 167ms/step - loss: 1.0369 - val_loss: 0.7313 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "272/272 [==============================] - 51s 187ms/step - loss: 1.0295 - val_loss: 0.7382 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "272/272 [==============================] - 51s 189ms/step - loss: 1.0229 - val_loss: 0.7286 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "272/272 [==============================] - 51s 188ms/step - loss: 1.0136 - val_loss: 0.7134 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "272/272 [==============================] - 46s 168ms/step - loss: 1.0061 - val_loss: 0.7083 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "272/272 [==============================] - 48s 175ms/step - loss: 0.9983 - val_loss: 0.7029 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "272/272 [==============================] - 52s 189ms/step - loss: 0.9924 - val_loss: 0.6963 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "272/272 [==============================] - 39s 142ms/step - loss: 0.9850 - val_loss: 0.6799 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "272/272 [==============================] - 49s 182ms/step - loss: 0.9769 - val_loss: 0.6815 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "272/272 [==============================] - 52s 189ms/step - loss: 0.9735 - val_loss: 0.6784 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "272/272 [==============================] - 39s 143ms/step - loss: 0.9654 - val_loss: 0.6782 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "272/272 [==============================] - 46s 168ms/step - loss: 0.9591 - val_loss: 0.6642 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "272/272 [==============================] - 47s 172ms/step - loss: 0.9531 - val_loss: 0.6609 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "272/272 [==============================] - 50s 185ms/step - loss: 0.9482 - val_loss: 0.6630 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "272/272 [==============================] - 51s 186ms/step - loss: 0.9426 - val_loss: 0.6623 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "272/272 [==============================] - 45s 167ms/step - loss: 0.9371 - val_loss: 0.6583 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "272/272 [==============================] - 52s 189ms/step - loss: 0.9316 - val_loss: 0.6465 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "272/272 [==============================] - 46s 167ms/step - loss: 0.9241 - val_loss: 0.6439 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "272/272 [==============================] - 46s 170ms/step - loss: 0.9201 - val_loss: 0.6522 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "272/272 [==============================] - 51s 186ms/step - loss: 0.9150 - val_loss: 0.6490 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "272/272 [==============================] - 45s 166ms/step - loss: 0.9107 - val_loss: 0.6385 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "272/272 [==============================] - 46s 167ms/step - loss: 0.9063 - val_loss: 0.6321 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "272/272 [==============================] - 47s 172ms/step - loss: 0.9015 - val_loss: 0.6249 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "272/272 [==============================] - 50s 185ms/step - loss: 0.8981 - val_loss: 0.6266 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "272/272 [==============================] - 52s 189ms/step - loss: 0.8929 - val_loss: 0.6132 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "272/272 [==============================] - 44s 164ms/step - loss: 0.8889 - val_loss: 0.6267 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "272/272 [==============================] - 49s 182ms/step - loss: 0.8860 - val_loss: 0.6192 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "272/272 [==============================] - 45s 166ms/step - loss: 0.8812 - val_loss: 0.6066 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "272/272 [==============================] - 49s 181ms/step - loss: 0.8790 - val_loss: 0.6066 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "272/272 [==============================] - 45s 165ms/step - loss: 0.8736 - val_loss: 0.6083 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "272/272 [==============================] - 48s 177ms/step - loss: 0.8688 - val_loss: 0.5985 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "272/272 [==============================] - 51s 186ms/step - loss: 0.8664 - val_loss: 0.6011 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "272/272 [==============================] - 45s 167ms/step - loss: 0.8628 - val_loss: 0.5897 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "272/272 [==============================] - 48s 177ms/step - loss: 0.8600 - val_loss: 0.5931 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "272/272 [==============================] - 51s 189ms/step - loss: 0.8559 - val_loss: 0.5841 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "272/272 [==============================] - 44s 163ms/step - loss: 0.8506 - val_loss: 0.5917 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "272/272 [==============================] - 46s 168ms/step - loss: 0.8481 - val_loss: 0.5827 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "272/272 [==============================] - 47s 171ms/step - loss: 0.8453 - val_loss: 0.5858 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "272/272 [==============================] - 51s 188ms/step - loss: 0.8424 - val_loss: 0.5803 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "272/272 [==============================] - 45s 165ms/step - loss: 0.8398 - val_loss: 0.5830 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "272/272 [==============================] - 49s 179ms/step - loss: 0.8364 - val_loss: 0.5783 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "272/272 [==============================] - 52s 190ms/step - loss: 0.8327 - val_loss: 0.5674 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "272/272 [==============================] - 45s 167ms/step - loss: 0.8300 - val_loss: 0.5611 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "272/272 [==============================] - 47s 173ms/step - loss: 0.8276 - val_loss: 0.5687 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "272/272 [==============================] - 51s 187ms/step - loss: 0.8240 - val_loss: 0.5660 - lr: 0.0010\n",
      "Epoch 70/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 46s 167ms/step - loss: 0.8222 - val_loss: 0.5680 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "272/272 [==============================] - 51s 186ms/step - loss: 0.8194 - val_loss: 0.5617 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "272/272 [==============================] - 45s 164ms/step - loss: 0.8173 - val_loss: 0.5618 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "272/272 [==============================] - 49s 182ms/step - loss: 0.8151 - val_loss: 0.5559 - lr: 0.0010\n",
      "Epoch 74/500\n",
      "272/272 [==============================] - 46s 167ms/step - loss: 0.8129 - val_loss: 0.5493 - lr: 0.0010\n",
      "Epoch 75/500\n",
      "272/272 [==============================] - 47s 174ms/step - loss: 0.8075 - val_loss: 0.5535 - lr: 0.0010\n",
      "Epoch 76/500\n",
      "272/272 [==============================] - 51s 187ms/step - loss: 0.8064 - val_loss: 0.5684 - lr: 0.0010\n",
      "Epoch 77/500\n",
      "272/272 [==============================] - 45s 167ms/step - loss: 0.8033 - val_loss: 0.5488 - lr: 0.0010\n",
      "Epoch 78/500\n",
      "272/272 [==============================] - 49s 182ms/step - loss: 0.7997 - val_loss: 0.5387 - lr: 0.0010\n",
      "Epoch 79/500\n",
      "272/272 [==============================] - 45s 165ms/step - loss: 0.8007 - val_loss: 0.5499 - lr: 0.0010\n",
      "Epoch 80/500\n",
      "272/272 [==============================] - 48s 177ms/step - loss: 0.7969 - val_loss: 0.5431 - lr: 0.0010\n",
      "Epoch 81/500\n",
      "272/272 [==============================] - 51s 187ms/step - loss: 0.7952 - val_loss: 0.5461 - lr: 0.0010\n",
      "Epoch 82/500\n",
      "272/272 [==============================] - 45s 167ms/step - loss: 0.7917 - val_loss: 0.5386 - lr: 0.0010\n",
      "Epoch 83/500\n",
      "272/272 [==============================] - 49s 180ms/step - loss: 0.7898 - val_loss: 0.5479 - lr: 0.0010\n",
      "Epoch 84/500\n",
      "272/272 [==============================] - 51s 189ms/step - loss: 0.7881 - val_loss: 0.5363 - lr: 0.0010\n",
      "Epoch 85/500\n",
      "272/272 [==============================] - 44s 163ms/step - loss: 0.7870 - val_loss: 0.5444 - lr: 0.0010\n",
      "Epoch 86/500\n",
      "272/272 [==============================] - 46s 167ms/step - loss: 0.7832 - val_loss: 0.5283 - lr: 0.0010\n",
      "Epoch 87/500\n",
      "272/272 [==============================] - 47s 173ms/step - loss: 0.7824 - val_loss: 0.5263 - lr: 0.0010\n",
      "Epoch 88/500\n",
      "272/272 [==============================] - 51s 186ms/step - loss: 0.7788 - val_loss: 0.5304 - lr: 0.0010\n",
      "Epoch 89/500\n",
      "272/272 [==============================] - 45s 164ms/step - loss: 0.7778 - val_loss: 0.5299 - lr: 0.0010\n",
      "Epoch 90/500\n",
      "272/272 [==============================] - 49s 178ms/step - loss: 0.7766 - val_loss: 0.5271 - lr: 0.0010\n",
      "Epoch 91/500\n",
      "272/272 [==============================] - 52s 190ms/step - loss: 0.7730 - val_loss: 0.5189 - lr: 0.0010\n",
      "Epoch 92/500\n",
      "272/272 [==============================] - 39s 143ms/step - loss: 0.7730 - val_loss: 0.5217 - lr: 0.0010\n",
      "Epoch 93/500\n",
      "272/272 [==============================] - 49s 181ms/step - loss: 0.7704 - val_loss: 0.5220 - lr: 0.0010\n",
      "Epoch 94/500\n",
      "272/272 [==============================] - 51s 186ms/step - loss: 0.7682 - val_loss: 0.5226 - lr: 0.0010\n",
      "Epoch 95/500\n",
      "272/272 [==============================] - 46s 168ms/step - loss: 0.7671 - val_loss: 0.5234 - lr: 0.0010\n",
      "Epoch 96/500\n",
      "272/272 [==============================] - 52s 190ms/step - loss: 0.7640 - val_loss: 0.5164 - lr: 0.0010\n",
      "Epoch 97/500\n",
      "272/272 [==============================] - 44s 163ms/step - loss: 0.7640 - val_loss: 0.5179 - lr: 0.0010\n",
      "Epoch 98/500\n",
      "272/272 [==============================] - 51s 187ms/step - loss: 0.7641 - val_loss: 0.5152 - lr: 0.0010\n",
      "Epoch 99/500\n",
      "272/272 [==============================] - 45s 165ms/step - loss: 0.7591 - val_loss: 0.5165 - lr: 0.0010\n",
      "Epoch 100/500\n",
      "272/272 [==============================] - 48s 178ms/step - loss: 0.7567 - val_loss: 0.5235 - lr: 0.0010\n",
      "Epoch 101/500\n",
      "272/272 [==============================] - 52s 190ms/step - loss: 0.7570 - val_loss: 0.5118 - lr: 0.0010\n",
      "Epoch 102/500\n",
      "272/272 [==============================] - 44s 163ms/step - loss: 0.7552 - val_loss: 0.5132 - lr: 0.0010\n",
      "Epoch 103/500\n",
      "272/272 [==============================] - 45s 164ms/step - loss: 0.7533 - val_loss: 0.5128 - lr: 0.0010\n",
      "Epoch 104/500\n",
      "272/272 [==============================] - 46s 169ms/step - loss: 0.7540 - val_loss: 0.5091 - lr: 0.0010\n",
      "Epoch 105/500\n",
      "272/272 [==============================] - 51s 188ms/step - loss: 0.7501 - val_loss: 0.5057 - lr: 0.0010\n",
      "Epoch 106/500\n",
      "272/272 [==============================] - 45s 165ms/step - loss: 0.7490 - val_loss: 0.5103 - lr: 0.0010\n",
      "Epoch 107/500\n",
      "272/272 [==============================] - 44s 163ms/step - loss: 0.7484 - val_loss: 0.5104 - lr: 0.0010\n",
      "Epoch 108/500\n",
      "272/272 [==============================] - 50s 184ms/step - loss: 0.7457 - val_loss: 0.5015 - lr: 0.0010\n",
      "Epoch 109/500\n",
      "272/272 [==============================] - 45s 165ms/step - loss: 0.7459 - val_loss: 0.5121 - lr: 0.0010\n",
      "Epoch 110/500\n",
      "272/272 [==============================] - 45s 167ms/step - loss: 0.7443 - val_loss: 0.5006 - lr: 0.0010\n",
      "Epoch 111/500\n",
      "272/272 [==============================] - 45s 165ms/step - loss: 0.7411 - val_loss: 0.5075 - lr: 0.0010\n",
      "Epoch 112/500\n",
      "272/272 [==============================] - 45s 165ms/step - loss: 0.7393 - val_loss: 0.5018 - lr: 0.0010\n",
      "Epoch 113/500\n",
      "272/272 [==============================] - 51s 188ms/step - loss: 0.7390 - val_loss: 0.4969 - lr: 0.0010\n",
      "Epoch 114/500\n",
      "272/272 [==============================] - 46s 168ms/step - loss: 0.7372 - val_loss: 0.4965 - lr: 0.0010\n",
      "Epoch 115/500\n",
      "272/272 [==============================] - 45s 166ms/step - loss: 0.7359 - val_loss: 0.4954 - lr: 0.0010\n",
      "Epoch 116/500\n",
      "272/272 [==============================] - 45s 165ms/step - loss: 0.7364 - val_loss: 0.4964 - lr: 0.0010\n",
      "Epoch 117/500\n",
      "272/272 [==============================] - 45s 164ms/step - loss: 0.7342 - val_loss: 0.4995 - lr: 0.0010\n",
      "Epoch 118/500\n",
      "272/272 [==============================] - 49s 179ms/step - loss: 0.7333 - val_loss: 0.4933 - lr: 0.0010\n",
      "Epoch 119/500\n",
      "272/272 [==============================] - 45s 166ms/step - loss: 0.7305 - val_loss: 0.4991 - lr: 0.0010\n",
      "Epoch 120/500\n",
      "272/272 [==============================] - 45s 167ms/step - loss: 0.7314 - val_loss: 0.4907 - lr: 0.0010\n",
      "Epoch 121/500\n",
      "272/272 [==============================] - 45s 165ms/step - loss: 0.7276 - val_loss: 0.4959 - lr: 0.0010\n",
      "Epoch 122/500\n",
      "272/272 [==============================] - 45s 164ms/step - loss: 0.7278 - val_loss: 0.4921 - lr: 0.0010\n",
      "Epoch 123/500\n",
      "272/272 [==============================] - 47s 172ms/step - loss: 0.7256 - val_loss: 0.4921 - lr: 0.0010\n",
      "Epoch 124/500\n",
      "272/272 [==============================] - 50s 182ms/step - loss: 0.7258 - val_loss: 0.4938 - lr: 0.0010\n",
      "Epoch 125/500\n",
      "272/272 [==============================] - 45s 167ms/step - loss: 0.7234 - val_loss: 0.4903 - lr: 0.0010\n",
      "Epoch 126/500\n",
      "272/272 [==============================] - 45s 166ms/step - loss: 0.7219 - val_loss: 0.4881 - lr: 0.0010\n",
      "Epoch 127/500\n",
      "272/272 [==============================] - 45s 165ms/step - loss: 0.7226 - val_loss: 0.4892 - lr: 0.0010\n",
      "Epoch 128/500\n",
      "272/272 [==============================] - 45s 167ms/step - loss: 0.7210 - val_loss: 0.4808 - lr: 0.0010\n",
      "Epoch 129/500\n",
      "272/272 [==============================] - 45s 165ms/step - loss: 0.7194 - val_loss: 0.4809 - lr: 0.0010\n",
      "Epoch 130/500\n",
      "272/272 [==============================] - 45s 165ms/step - loss: 0.7183 - val_loss: 0.4857 - lr: 0.0010\n",
      "Epoch 131/500\n",
      "272/272 [==============================] - 46s 168ms/step - loss: 0.7172 - val_loss: 0.4806 - lr: 0.0010\n",
      "Epoch 132/500\n",
      "272/272 [==============================] - 47s 174ms/step - loss: 0.7157 - val_loss: 0.4757 - lr: 0.0010\n",
      "Epoch 133/500\n",
      "272/272 [==============================] - 46s 168ms/step - loss: 0.7155 - val_loss: 0.4820 - lr: 0.0010\n",
      "Epoch 134/500\n",
      "272/272 [==============================] - 45s 167ms/step - loss: 0.7145 - val_loss: 0.4801 - lr: 0.0010\n",
      "Epoch 135/500\n",
      "272/272 [==============================] - 45s 165ms/step - loss: 0.7138 - val_loss: 0.4911 - lr: 0.0010\n",
      "Epoch 136/500\n",
      "272/272 [==============================] - 45s 165ms/step - loss: 0.7123 - val_loss: 0.4794 - lr: 0.0010\n",
      "Epoch 137/500\n",
      "272/272 [==============================] - 46s 169ms/step - loss: 0.7113 - val_loss: 0.4755 - lr: 0.0010\n",
      "Epoch 138/500\n",
      "272/272 [==============================] - 45s 165ms/step - loss: 0.7091 - val_loss: 0.4879 - lr: 0.0010\n",
      "Epoch 139/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 45s 165ms/step - loss: 0.7087 - val_loss: 0.4842 - lr: 0.0010\n",
      "Epoch 140/500\n",
      "272/272 [==============================] - 45s 165ms/step - loss: 0.7070 - val_loss: 0.4853 - lr: 0.0010\n",
      "Epoch 141/500\n",
      "272/272 [==============================] - 45s 166ms/step - loss: 0.7071 - val_loss: 0.4784 - lr: 0.0010\n",
      "Epoch 142/500\n",
      "272/272 [==============================] - 45s 165ms/step - loss: 0.7059 - val_loss: 0.4804 - lr: 0.0010\n",
      "Epoch 143/500\n",
      "272/272 [==============================] - 45s 165ms/step - loss: 0.7062 - val_loss: 0.4796 - lr: 0.0010\n",
      "Epoch 144/500\n",
      "272/272 [==============================] - 45s 165ms/step - loss: 0.7033 - val_loss: 0.4782 - lr: 0.0010\n",
      "Epoch 145/500\n",
      "272/272 [==============================] - 45s 166ms/step - loss: 0.7042 - val_loss: 0.4855 - lr: 0.0010\n",
      "Epoch 146/500\n",
      "272/272 [==============================] - 45s 166ms/step - loss: 0.7020 - val_loss: 0.4769 - lr: 0.0010\n",
      "Epoch 147/500\n",
      "272/272 [==============================] - 45s 165ms/step - loss: 0.7008 - val_loss: 0.4811 - lr: 0.0010\n",
      "f1-scores\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-859222e2018f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_deep_learning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Coding_projects\\Thai_NLP\\text_classification.py\u001b[0m in \u001b[0;36mfit_deep_learning\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    447\u001b[0m           class_weight=self.class_weight)\n\u001b[0;32m    448\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f1-scores'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_dl_text_classification\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Val_loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Coding_projects\\Thai_NLP\\text_classification.py\u001b[0m in \u001b[0;36meval_dl_text_classification\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    390\u001b[0m       \u001b[0my_true_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m       \u001b[0my_pred_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m       \u001b[0mf1s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m       \u001b[0mW_f1s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weighted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m       \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'f1s: {f1s}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\text_bay\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\text_bay\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1045\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m                        \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m                        zero_division=zero_division)\n\u001b[0m\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\text_bay\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\text_bay\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1173\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f-score'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m                                                  \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1175\u001b[1;33m                                                  zero_division=zero_division)\n\u001b[0m\u001b[0;32m   1176\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\text_bay\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\text_bay\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1432\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1433\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[1;32m-> 1434\u001b[1;33m                                     pos_label)\n\u001b[0m\u001b[0;32m   1435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1436\u001b[0m     \u001b[1;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\text_bay\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1263\u001b[0m             raise ValueError(\"Target is %s but average='binary'. Please \"\n\u001b[0;32m   1264\u001b[0m                              \u001b[1;34m\"choose another average setting, one of %r.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1265\u001b[1;33m                              % (y_type, average_options))\n\u001b[0m\u001b[0;32m   1266\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m         warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "\u001b[1;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "tc.fit_deep_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n",
      "[[[ 1  2  3]\n",
      "  [ 1 12  7]]\n",
      "\n",
      " [[ 3  2  1]\n",
      "  [ 2  2  2]]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[[1,2,3],[1,12,7]],[[3,2,1],[2,2,2]]])\n",
    "print(a.shape)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(a,axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  1, 12,  7,  3,  2,  1,  2,  2,  2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_bay",
   "language": "python",
   "name": "text_bay"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
