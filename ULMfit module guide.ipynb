{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pythainlp.ulmfit import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ulmfit import ULMfit_model, ULMfit_for_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test_RS_123.csv')\n",
    "df = df.dropna().reset_index(drop=True).iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>สำหรับบริการเยี่ยมมาก</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ถ้าไม่ค้าง ข้อมูลบูโรก็ขึ้นปกติ</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ครบแล้ว ครบถ้วนมากเลย</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ทำระยำอะไรไว้เนี้ย</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>เกือบสี่โมงเย็นโทรมายืนยันอีกที่รับสายทันถามข้...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts    labels\n",
       "0                              สำหรับบริการเยี่ยมมาก  positive\n",
       "1                    ถ้าไม่ค้าง ข้อมูลบูโรก็ขึ้นปกติ   neutral\n",
       "2                              ครบแล้ว ครบถ้วนมากเลย  positive\n",
       "3                                 ทำระยำอะไรไว้เนี้ย  negative\n",
       "4  เกือบสี่โมงเย็นโทรมายืนยันอีกที่รับสายทันถามข้...   neutral"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(a, dtype=dtype, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples: 80\n",
      "number of test samples: 20\n"
     ]
    }
   ],
   "source": [
    "model_path = 'Untitled Folder'\n",
    "# The code will split df into 80:20 train:test split\n",
    "model = ULMfit_model(df, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training unfrozen\n",
      "Max fitting loop is 1000 ==> EarlyStopping will automatically terminate the training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='58' class='' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      5.80% [58/1000 26:50<7:15:53]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.550715</td>\n",
       "      <td>3.896815</td>\n",
       "      <td>0.217560</td>\n",
       "      <td>01:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.529984</td>\n",
       "      <td>3.843709</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.521069</td>\n",
       "      <td>3.774970</td>\n",
       "      <td>0.234821</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.503013</td>\n",
       "      <td>3.671808</td>\n",
       "      <td>0.255060</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.491458</td>\n",
       "      <td>3.596232</td>\n",
       "      <td>0.267560</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.484786</td>\n",
       "      <td>3.624085</td>\n",
       "      <td>0.269345</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.457481</td>\n",
       "      <td>3.502994</td>\n",
       "      <td>0.283929</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.425003</td>\n",
       "      <td>3.431190</td>\n",
       "      <td>0.303274</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.400083</td>\n",
       "      <td>3.403070</td>\n",
       "      <td>0.317560</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.368537</td>\n",
       "      <td>3.341419</td>\n",
       "      <td>0.333929</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.344018</td>\n",
       "      <td>3.313733</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.317456</td>\n",
       "      <td>3.245309</td>\n",
       "      <td>0.365179</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.293382</td>\n",
       "      <td>3.207881</td>\n",
       "      <td>0.383036</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.269459</td>\n",
       "      <td>3.151958</td>\n",
       "      <td>0.392262</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>4.248313</td>\n",
       "      <td>3.115453</td>\n",
       "      <td>0.404167</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.220966</td>\n",
       "      <td>3.080745</td>\n",
       "      <td>0.417262</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>4.196158</td>\n",
       "      <td>3.044387</td>\n",
       "      <td>0.427381</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>4.171982</td>\n",
       "      <td>2.996271</td>\n",
       "      <td>0.439881</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>4.146763</td>\n",
       "      <td>2.963523</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>4.124109</td>\n",
       "      <td>2.941947</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.099667</td>\n",
       "      <td>2.911969</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>4.076671</td>\n",
       "      <td>2.865978</td>\n",
       "      <td>0.466071</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>4.053448</td>\n",
       "      <td>2.843261</td>\n",
       "      <td>0.462798</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>4.032846</td>\n",
       "      <td>2.823918</td>\n",
       "      <td>0.462202</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>4.009169</td>\n",
       "      <td>2.800245</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.987118</td>\n",
       "      <td>2.804260</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.963537</td>\n",
       "      <td>2.797732</td>\n",
       "      <td>0.440476</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.939924</td>\n",
       "      <td>2.783142</td>\n",
       "      <td>0.416071</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.915576</td>\n",
       "      <td>2.795243</td>\n",
       "      <td>0.397321</td>\n",
       "      <td>00:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.905072</td>\n",
       "      <td>2.781972</td>\n",
       "      <td>0.411905</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.883077</td>\n",
       "      <td>2.817931</td>\n",
       "      <td>0.367262</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.859686</td>\n",
       "      <td>2.818176</td>\n",
       "      <td>0.366071</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.836684</td>\n",
       "      <td>2.770074</td>\n",
       "      <td>0.366964</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.815408</td>\n",
       "      <td>2.727117</td>\n",
       "      <td>0.379464</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.791359</td>\n",
       "      <td>2.685399</td>\n",
       "      <td>0.393750</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.768102</td>\n",
       "      <td>2.663458</td>\n",
       "      <td>0.405952</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.744375</td>\n",
       "      <td>2.644268</td>\n",
       "      <td>0.427083</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.722074</td>\n",
       "      <td>2.627620</td>\n",
       "      <td>0.445833</td>\n",
       "      <td>00:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>3.699995</td>\n",
       "      <td>2.622341</td>\n",
       "      <td>0.443452</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.676222</td>\n",
       "      <td>2.609826</td>\n",
       "      <td>0.434821</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.654472</td>\n",
       "      <td>2.595629</td>\n",
       "      <td>0.440476</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>3.632013</td>\n",
       "      <td>2.593039</td>\n",
       "      <td>0.422321</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>3.621089</td>\n",
       "      <td>2.615525</td>\n",
       "      <td>0.380060</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>3.598284</td>\n",
       "      <td>2.579388</td>\n",
       "      <td>0.397321</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.574958</td>\n",
       "      <td>2.582124</td>\n",
       "      <td>0.393750</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.551967</td>\n",
       "      <td>2.564341</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>3.528614</td>\n",
       "      <td>2.565090</td>\n",
       "      <td>0.393750</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>3.504290</td>\n",
       "      <td>2.556958</td>\n",
       "      <td>0.393452</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.481122</td>\n",
       "      <td>2.550776</td>\n",
       "      <td>0.398214</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>3.457484</td>\n",
       "      <td>2.544740</td>\n",
       "      <td>0.399702</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.432988</td>\n",
       "      <td>2.531286</td>\n",
       "      <td>0.411012</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>3.408958</td>\n",
       "      <td>2.528368</td>\n",
       "      <td>0.407738</td>\n",
       "      <td>00:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>3.385058</td>\n",
       "      <td>2.515932</td>\n",
       "      <td>0.410417</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>3.360327</td>\n",
       "      <td>2.517747</td>\n",
       "      <td>0.408631</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>3.336404</td>\n",
       "      <td>2.518234</td>\n",
       "      <td>0.399107</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>3.309876</td>\n",
       "      <td>2.516040</td>\n",
       "      <td>0.400595</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>3.292210</td>\n",
       "      <td>2.539835</td>\n",
       "      <td>0.387798</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>3.266607</td>\n",
       "      <td>2.521829</td>\n",
       "      <td>0.396131</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:04<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 3.8968145847320557.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 1 with valid_loss value: 3.8437092304229736.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 2 with valid_loss value: 3.774970054626465.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 3 with valid_loss value: 3.6718084812164307.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 4 with valid_loss value: 3.5962321758270264.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n",
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 6 with valid_loss value: 3.502993583679199.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 7 with valid_loss value: 3.4311904907226562.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 8 with valid_loss value: 3.4030697345733643.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 9 with valid_loss value: 3.341418981552124.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 10 with valid_loss value: 3.3137333393096924.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 11 with valid_loss value: 3.2453091144561768.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 12 with valid_loss value: 3.207880973815918.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 13 with valid_loss value: 3.1519575119018555.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 14 with valid_loss value: 3.115453004837036.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 15 with valid_loss value: 3.080744743347168.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 16 with valid_loss value: 3.044386863708496.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 17 with valid_loss value: 2.9962713718414307.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 18 with valid_loss value: 2.9635231494903564.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 19 with valid_loss value: 2.9419467449188232.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 20 with valid_loss value: 2.911968946456909.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 21 with valid_loss value: 2.8659775257110596.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 22 with valid_loss value: 2.843261480331421.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 23 with valid_loss value: 2.823918104171753.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 24 with valid_loss value: 2.8002450466156006.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n",
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 26 with valid_loss value: 2.79773211479187.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 27 with valid_loss value: 2.783142328262329.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n",
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 29 with valid_loss value: 2.7819719314575195.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n",
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n",
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 32 with valid_loss value: 2.770073890686035.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 33 with valid_loss value: 2.727116823196411.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 34 with valid_loss value: 2.685399293899536.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 35 with valid_loss value: 2.6634583473205566.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 36 with valid_loss value: 2.644268274307251.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 37 with valid_loss value: 2.627619504928589.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 38 with valid_loss value: 2.6223411560058594.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 39 with valid_loss value: 2.60982608795166.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 40 with valid_loss value: 2.5956287384033203.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 41 with valid_loss value: 2.593039035797119.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n",
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 43 with valid_loss value: 2.579387903213501.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n",
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 45 with valid_loss value: 2.5643413066864014.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n",
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 47 with valid_loss value: 2.556957721710205.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 48 with valid_loss value: 2.550776243209839.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 49 with valid_loss value: 2.5447404384613037.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 50 with valid_loss value: 2.5312857627868652.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 51 with valid_loss value: 2.5283684730529785.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 52 with valid_loss value: 2.5159318447113037.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n",
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n",
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n",
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n",
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n",
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train language model (Unsupervised learning)\n",
    "model.fit_lm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamin/.virtualenv/dev36/lib/python3.6/site-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(a, dtype=dtype, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total vocab size: 120\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.308604</td>\n",
       "      <td>1.123861</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.150034</td>\n",
       "      <td>1.125018</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.075247</td>\n",
       "      <td>1.126109</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max fitting loop is 1000 ==> EarlyStopping will automatically terminate the training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='6' class='' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.60% [6/1000 01:51<5:07:26]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.117586</td>\n",
       "      <td>1.127736</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.104123</td>\n",
       "      <td>1.128400</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.138974</td>\n",
       "      <td>1.129028</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.156873</td>\n",
       "      <td>1.129416</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.140657</td>\n",
       "      <td>1.128272</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.137552</td>\n",
       "      <td>1.129552</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:01<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with accuracy value: 0.25.\n",
      "Epoch 6: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-scores for each class: [0.181818 0.       0.380952]\n",
      "Weighted avg f1-score: 0.1588744588744589, Unweighted avf f1-score: 0.1875901875901876\n"
     ]
    }
   ],
   "source": [
    "df = df.sample(frac=1.0).reset_index(drop=True)\n",
    "#split data to 80:20\n",
    "#supervised training ==> in this case, sentiment analysis\n",
    "model.fit_classifier(df.iloc[:int(len(df)*0.8)].reset_index(drop=True),\n",
    "                     df.iloc[int(len(df)*0.8):].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export everything we need for deployment into 1 file (.pkl extension)\n",
    "#this file will be saved in the model_path folder\n",
    "model.export_classifier('ULM_sentiment_v1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#the class ULMfit_for_predict requires model_folder_name, model_name and min_char_len\n",
    "model_folder_name = 'ULMfit_RS_123/Untitled Folder'\n",
    "model_name = 'ULMfit_sentiment_v1.pkl'#'ULM_sentiment_v1.pkl'\n",
    "#the above parameters mean that our pretrained model is located at ./ULMfit_RS_123/Untitled Folder/ULMfit_sentiment_v1.pkl\n",
    "#we dont need other files except this .pkl file (feel free to remove other files, if you want to)\n",
    "min_char_len = 0\n",
    "model2 = ULMfit_for_predict(model_folder_name=model_folder_name,\n",
    "                            model_name = model_name,\n",
    "                            min_char_len = min_char_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 0.009009539, 'neutral': 0.98944074, 'positive': 0.0015498002}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict('aaaaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 0.8455254, 'neutral': 0.15344772, 'positive': 0.0010268837}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict('ทำระยำอะไรไว้เนี้ย')# real case from CS team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 0.000551239, 'neutral': 0.00012218668, 'positive': 0.9993266}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict('โอเคร ครบถ้วนมากเลย')# real case from CS team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 0.74568003, 'neutral': 0.24938914, 'positive': 0.004930807}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict('ฉันจะหักบัตรทิ้ง ไม่เอาแล้ว')# Generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 0.66031706, 'neutral': 0.32772264, 'positive': 0.0119603425}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict('ให้ฉันรอมา 2 อาทิตย์ นานเกินไปมั้ยอะ')# Generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 0.88219035, 'neutral': 0.114249274, 'positive': 0.0035603484}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict('มันแย่มากเลยจริงๆ')# Generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 0.96117854, 'neutral': 0.030168844, 'positive': 0.008652666}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict('เสียเวลามากๆ คุณทำงานยังไงกันเนี่ย')# Generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 0.9469962, 'neutral': 0.050073743, 'positive': 0.0029300505}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict('ทำไมถึงต้องเรื่องมากขนาดนี้ฉันไม่เข้าใจเลย')# real case from CS team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 0.24441269, 'neutral': 0.25831106, 'positive': 0.49727634}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict('เออ แบบนี้ก็โอเครนะ ดีเหมือนกัน')# Generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 0.7122914, 'neutral': 0.27952793, 'positive': 0.008180632}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict('ทำไมบัตรธนาคารอื่นเขาไม่เห็นจะเรื่องมากเหมือนบัตรคุณเลย')# real case from CS team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 0.55406606, 'neutral': 0.37786, 'positive': 0.068073906}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict('คุณเอาอะไรคิด')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 0.01816207, 'neutral': 0.012729647, 'positive': 0.9691083}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict('ดี เห็นภาพเลย')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 0.015407715, 'neutral': 0.010374289, 'positive': 0.97421795}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict('โอเครเลย')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 0.6946717, 'neutral': 0.041687615, 'positive': 0.26364067}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict('อธิบายไม่เคลียร์เลยจริงๆ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result_filtered_july.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong type: neutral_wrong_pred\n",
      "sample No.: 0, text: okคุยกันแล้วดีเนอะ เข้าใจดี\n",
      "sample No.: 1, text: ถ้าเจอคนแบบคุณทุกครั้งที่โทรเข้ามา ประเทศชาติคงเจริญ\n",
      "sample No.: 2, text: เพอร์เฟค\n",
      "sample No.: 3, text: ขอบคุณที่ให้คำตอบเป็นอย่างดีนะครับ\n",
      "sample No.: 4, text: แค่นี้ก้ช่วยได้เยอะแล้ว\n",
      "sample No.: 5, text: ลูกค้าชม เห็นภาพ\n",
      "--------------------------\n",
      "wrong type: negative_wrong_pred\n",
      "sample No.: 0, text: ดีจังเลย รู้แบบนี้ ติดต่อที่กรุงศรีแต่แรกก็ดี ทำไมธนาคารอื่นไม่เห็นบริการแบบนี้\n",
      "sample No.: 1, text: ก่อนหน้าโทรมาเค้าไม่เห็นแนะนำแบบนี้เลย แบบนี้น่าจะดีกว่า\n",
      "sample No.: 2, text: เคลียร์จริงๆ เข้าใจล่ะ\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in data:\n",
    "    print(f'wrong type: {i}')\n",
    "    for ind, j in enumerate(data[i]):\n",
    "        print(f'sample No.: {ind}, text: {j}')\n",
    "    print('--------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# computing score (July)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('compliment_july_filtered.txt', 'r', encoding='utf8') as f:\n",
    "    for i in f:\n",
    "        data.append(i.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "labels = ['positive']*len(data)\n",
    "y_pred = []\n",
    "for i in data:\n",
    "    tmp = model2.predict(i)\n",
    "    tmp = sorted(tmp.items(), key = lambda x: x[1], reverse = True)\n",
    "    y_pred.append(tmp[0][0])\n",
    "conf_mat = confusion_matrix(labels,y_pred)\n",
    "sns.heatmap(conf_mat, annot=True, fmt=\"d\",\n",
    "            xticklabels=model2.classes, yticklabels=model2.classes)\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "labels = ['positive']*len(data)\n",
    "y_pred = []\n",
    "for i in data:\n",
    "    tmp = model2.predict(i)\n",
    "    tmp = sorted(tmp.items(), key = lambda x: x[1], reverse = True)\n",
    "    y_pred.append(tmp[0][0])\n",
    "conf_mat = confusion_matrix(labels,y_pred)\n",
    "sns.heatmap(conf_mat, annot=True, fmt=\"d\",\n",
    "            xticklabels=model2.classes, yticklabels=model2.classes)\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9575471698113207\n",
      "0.014150943396226415\n",
      "0.02830188679245283\n"
     ]
    }
   ],
   "source": [
    "print(203/(212))\n",
    "print(3/(212))\n",
    "print(6/(212))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8732394366197183\n",
      "0.07981220657276995\n",
      "0.046948356807511735\n"
     ]
    }
   ],
   "source": [
    "print(186/(17+10+186))\n",
    "print(17/(17+10+186))\n",
    "print(10/(17+10+186))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('negative', 0.0015353437), ('neutral', 0.00078217295), ('positive', 0.99768245)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'neutral', 'positive']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred label: neutral, text: okคุยกันแล้วดีเนอะ เข้าใจดี\n",
      "pred label: negative, text: ดีจังเลย รู้แบบนี้ ติดต่อที่กรุงศรีแต่แรกก็ดี ทำไมธนาคารอื่นไม่เห็นบริการแบบนี้\n",
      "pred label: neutral, text: ถ้าเจอคนแบบคุณทุกครั้งที่โทรเข้ามา ประเทศชาติคงเจริญ\n",
      "pred label: neutral, text: เพอร์เฟค\n",
      "pred label: neutral, text: ขอบคุณที่ให้คำตอบเป็นอย่างดีนะครับ\n",
      "pred label: neutral, text: แค่นี้ก้ช่วยได้เยอะแล้ว\n",
      "pred label: negative, text: ก่อนหน้าโทรมาเค้าไม่เห็นแนะนำแบบนี้เลย แบบนี้น่าจะดีกว่า\n",
      "pred label: negative, text: เคลียร์จริงๆ เข้าใจล่ะ\n",
      "pred label: neutral, text: ลูกค้าชม เห็นภาพ\n"
     ]
    }
   ],
   "source": [
    "negative_wrong_pred = []\n",
    "neutral_wrong_pred = []\n",
    "for ind, i  in enumerate(y_pred):\n",
    "    if i != 'positive':\n",
    "        print(f'pred label: {y_pred[ind]}, text: {data[ind]}')\n",
    "        if i == 'negative':\n",
    "            negative_wrong_pred.append(data[ind])\n",
    "        else:\n",
    "            neutral_wrong_pred.append(data[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ดีจังเลย รู้แบบนี้ ติดต่อที่กรุงศรีแต่แรกก็ดี ทำไมธนาคารอื่นไม่เห็นบริการแบบนี้',\n",
       " 'ก่อนหน้าโทรมาเค้าไม่เห็นแนะนำแบบนี้เลย แบบนี้น่าจะดีกว่า',\n",
       " 'เคลียร์จริงๆ เข้าใจล่ะ']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_wrong_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['okคุยกันแล้วดีเนอะ เข้าใจดี',\n",
       " 'ถ้าเจอคนแบบคุณทุกครั้งที่โทรเข้ามา ประเทศชาติคงเจริญ',\n",
       " 'เพอร์เฟค',\n",
       " 'ขอบคุณที่ให้คำตอบเป็นอย่างดีนะครับ',\n",
       " 'แค่นี้ก้ช่วยได้เยอะแล้ว',\n",
       " 'ลูกค้าชม เห็นภาพ']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutral_wrong_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('result_filtered_july.json', 'w', encoding='utf8') as f:\n",
    "    json.dump({'neutral_wrong_pred':neutral_wrong_pred, 'negative_wrong_pred': negative_wrong_pred},f,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
